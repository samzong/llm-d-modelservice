# generated by generate-example-output.sh
---
# Source: llm-d-modelservice/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: requester-llm-d-modelservice
  labels:
    helm.sh/chart: llm-d-modelservice-v0.3.6
    app.kubernetes.io/version: "v0.2.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: llm-d-modelservice/templates/decode-requester-replicaset.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: requester-llm-d-modelservice-decode
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dp-app
  template:
    metadata:
      labels:
        app: dp-app
      annotations:
        dual-pod.llm-d.ai/admin-port: "8081"
        dual-pod.llm-d.ai/server-patch: |
          metadata:
            labels: {"model-reg": "opt-125m",
              "model-repo": "facebook",
              "app": null}
          spec:
            initContainers:
              - name: routing-proxy
                args:
                  - --port=8000
                  - --vllm-port=8200
                  - --connector=nixlv2
                  - -v=5
                  - --secure-proxy=false
                image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.3.0
                imagePullPolicy: Always
                ports:
                  - containerPort: 8000
                resources: {}
                restartPolicy: Always
                securityContext:
                  allowPrivilegeEscalation: false
                  runAsNonRoot: true
            containers:
              - name: vllm
                image: ghcr.io/llm-d/llm-d:v0.2.0
                
                command: ["vllm", "serve"]
                args:
                  - "facebook/opt-125m"
                  - --port
                  - "8200"
                  - --served-model-name
                  - "facebook/opt-125m"
                  
                  - --enforce-eager
                  - --kv-transfer-config
                  - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
                env:
                - name: CUDA_VISIBLE_DEVICES
                  value: "0"
                - name: UCX_TLS
                  value: cuda_ipc,cuda_copy,tcp
                - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                  value: "5557"
                - name: VLLM_LOGGING_LEVEL
                  value: DEBUG
                - name: DP_SIZE
                  value: "1"
                - name: TP_SIZE
                  value: "1"
                
                - name: HF_HOME
                  value: /model-cache
                ports:
                - containerPort: 8200
                  protocol: TCP
                - containerPort: 5557
                  protocol: TCP
                
                resources:
                  limits:
                    cpu: "16"
                    memory: 16Gi
                    nvidia.com/gpu: "1"
                  requests:
                    cpu: "16"
                    memory: 16Gi
                    nvidia.com/gpu: "1"
                
                volumeMounts:
                  - name: model-storage
                    mountPath: /model-cache
    spec:
      containers:
        - name: inference-server
          image: ghcr.io/llm-d-incubation/llm-d-fast-model-actuation-requester:latest
          imagePullPolicy: Always
          command:
          - /app/requester
          env:
          - name: PROBES_PORT
            value: "8080"
          - name: SPI_PORT
            value: "8081"
          ports:
          - name: probes
            containerPort: 8080
          - name: spi
            containerPort: 8081
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 2
            periodSeconds: 5
          resources:
            limits:
              nvidia.com/gpu: 1
              cpu: 1
              memory: 250Mi
---
# Source: llm-d-modelservice/templates/prefill-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: requester-llm-d-modelservice-prefill
  labels:
    helm.sh/chart: llm-d-modelservice-v0.3.6
    app.kubernetes.io/version: "v0.2.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: facebook-opt-125m
      llm-d.ai/role: prefill
  template:
    metadata:
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: facebook-opt-125m
        llm-d.ai/role: prefill
    spec:
    
      serviceAccountName: requester-llm-d-modelservice
      
      volumes:
        - emptyDir: {}
          name: metrics-volume
        - name: model-storage
          emptyDir:
            sizeLimit: 20Gi
        
      containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d:v0.2.0
          
          command: ["vllm", "serve"]
          args:
            - "facebook/opt-125m"
            - --port
            - "8000"
            - --served-model-name
            - "facebook/opt-125m"
            
            - --enforce-eager
            - --kv-transfer-config
            - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
          env:
          - name: CUDA_VISIBLE_DEVICES
            value: "0"
          - name: UCX_TLS
            value: cuda_ipc,cuda_copy,tcp
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: "5557"
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          - name: DP_SIZE
            value: "1"
          - name: TP_SIZE
            value: "1"
          
          - name: HF_HOME
            value: /model-cache
          ports:
          - containerPort: 8000
            protocol: TCP
          - containerPort: 5557
            protocol: TCP
          
          resources:
            limits:
              cpu: "16"
              memory: 16Gi
              nvidia.com/gpu: "1"
            requests:
              cpu: "16"
              memory: 16Gi
              nvidia.com/gpu: "1"
          
          volumeMounts:
            - name: model-storage
              mountPath: /model-cache
